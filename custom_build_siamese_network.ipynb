{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pandas\n",
    "!pip install --timeout=600 tensorflow[and-cuda]\n",
    "!pip install numpy \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import random\n",
    "import tensorflow as tf\n",
    "from keras.applications.efficientnet import EfficientNetB0\n",
    "from PIL import Image\n",
    "import numpy as np \n",
    "import keras.backend as K\n",
    "from keras import metrics,layers,models\n",
    "import tqdm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> GPU Testing Tensorflow Compatibility </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Check if GPU is available\n",
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\n",
    "\n",
    "# Print the GPU devices\n",
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "if len(physical_devices) > 0:\n",
    "    print(\"GPU Device Name:\", physical_devices[0].name)\n",
    "else:\n",
    "    print(\"No GPU devices found.\")\n",
    "\n",
    "\n",
    "    # Check devices\n",
    "devices = tf.config.list_physical_devices()\n",
    "print(devices)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> IDK WHAT THIS IS </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Flatten, Dense, Input, Concatenate, Lambda\n",
    "from tensorflow.keras.applications import EfficientNetB0\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.applications.efficientnet import preprocess_input\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np \n",
    "from PIL import Image\n",
    "import os\n",
    "import random\n",
    "import pandas as pd\n",
    "\n",
    "# Assuming you have a function to convert an image to a tensor\n",
    "def convert_image_to_tensor(path):\n",
    "    img = Image.open(path)\n",
    "    img = img.resize((224, 224))\n",
    "    img = np.asarray(img)\n",
    "    return img\n",
    "\n",
    "def get_files_list(path_of_file):\n",
    "    file_list = os.listdir(path_of_file)\n",
    "    file_list = [os.path.join(path_of_file, i) for i in file_list]\n",
    "    shuffled_file_list = [item for item in file_list for _ in range(1)]\n",
    "    random.shuffle(shuffled_file_list)\n",
    "    for f in shuffled_file_list:\n",
    "        file_list.append(f)\n",
    "        \n",
    "    return file_list\n",
    "\n",
    "left_images_path = 'left/left/'  # path for all left side images\n",
    "right_images_path = 'right/right/'  # path for all right side images\n",
    "data_set_raw = pd.DataFrame(columns=['left_image', 'right_image', 'label'])\n",
    "\n",
    "left_images = get_files_list(left_images_path)\n",
    "right_images = get_files_list(right_images_path)\n",
    "\n",
    "data_set_raw['left_image'] = left_images\n",
    "data_set_raw['right_image'] = right_images\n",
    "\n",
    "data_set_raw['label'] = 0\n",
    "data_set_raw.loc[data_set_raw['left_image'].apply(lambda x: x.split('/')[-1]) == data_set_raw['right_image'].apply(lambda x: x.split('/')[-1]), 'label'] = 1\n",
    "\n",
    "X_left_paths = data_set_raw['left_image']\n",
    "X_right_paths = data_set_raw['right_image']\n",
    "Y = np.array(data_set_raw['label'].to_list()).reshape(-1, 1)\n",
    "\n",
    "# Split the data into training and validation sets\n",
    "X_left_train, X_left_val, X_right_train, X_right_val, Y_train, Y_val = train_test_split(\n",
    "    X_left_paths, X_right_paths, Y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Input layers\n",
    "input_left = Input(shape=(224, 224, 3))\n",
    "input_right = Input(shape=(224, 224, 3))\n",
    "\n",
    "# Build Siamese model\n",
    "def build_siamese_model():\n",
    "    input_val = Input(shape=(224, 224, 3))\n",
    "    base_model = EfficientNetB0(include_top=False, input_tensor=input_val, weights='imagenet')\n",
    "\n",
    "    for layer in base_model.layers[:int(len(base_model.layers) * 0.75)]:\n",
    "        layer.trainable = False\n",
    "\n",
    "    encoded_left = base_model(input_left)\n",
    "    encoded_right = base_model(input_right)\n",
    "\n",
    "    flat_encoded_left = Flatten()(encoded_left)\n",
    "    flat_encoded_right = Flatten()(encoded_right)\n",
    "\n",
    "    distance = Lambda(lambda tensors: tf.abs(tensors[0] - tensors[1]))([flat_encoded_left, flat_encoded_right])\n",
    "    dense_output = Dense(1, activation='sigmoid')(distance)\n",
    "\n",
    "    model = tf.keras.models.Model(inputs=[input_left, input_right], outputs=dense_output)\n",
    "    return model\n",
    "\n",
    "# Create the Siamese model\n",
    "siamese_model = build_siamese_model()\n",
    "\n",
    "# Compile the model with contrastive loss\n",
    "siamese_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Display the model summary\n",
    "siamese_model.summary()\n",
    "\n",
    "# Train the Siamese model over batches\n",
    "batch_size = 64  # Set your desired batch size\n",
    "total_epochs = 20  # Set the total number of epochs\n",
    "\n",
    "try:\n",
    "    siamese_model.load_weights('siamese_final.h5')  # Adjust the filename based on your requirements\n",
    "    print('Loaded Model')\n",
    "    starting_epoch=5\n",
    "except:\n",
    "    print('Resetting from Scratch')\n",
    "    starting_epoch=0\n",
    "    \n",
    "\n",
    "\n",
    "for epoch in range(total_epochs):\n",
    "    print(f'Epoch Number: {epoch + 1}')\n",
    "    \n",
    "    # Create a generator for labels\n",
    "    label_generator = iter(Y_train)\n",
    "    \n",
    "    # Calculate the number of batches\n",
    "    num_batches = len(X_left_train) // batch_size\n",
    "    \n",
    "    # Iterate over batches\n",
    "    for i in range(num_batches):\n",
    "        # Extract a batch of left and right images\n",
    "        left_batch_paths = X_left_train[i * batch_size: (i + 1) * batch_size]\n",
    "        right_batch_paths = X_right_train[i * batch_size: (i + 1) * batch_size]\n",
    "\n",
    "        # Load and preprocess images\n",
    "        left_batch = np.array([convert_image_to_tensor(img_path) for img_path in left_batch_paths])\n",
    "        right_batch = np.array([convert_image_to_tensor(img_path) for img_path in right_batch_paths])\n",
    "\n",
    "        # Extract labels for the current batch\n",
    "        label_batch = np.array([next(label_generator) for _ in range(batch_size)])\n",
    "\n",
    "        # Train the Siamese model\n",
    "        history = siamese_model.train_on_batch([left_batch, right_batch], label_batch)\n",
    "\n",
    "        # Print batch-wise metrics\n",
    "        print(f'Batch {i + 1}/{num_batches} - Loss: {history[0]}, Accuracy: {history[1]}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_image_to_tensor(path,show_flag=0):\n",
    "    img = Image.open(path)\n",
    "    img = img.resize((224, 224))\n",
    "    if show_flag==1:\n",
    "        display(img)\n",
    "    img = np.asarray(img)\n",
    "    return img\n",
    "\n",
    "\n",
    "siamese_model.predict([convert_image_to_tensor('left/exp_4.jpg',1).reshape(1,224,224,3),convert_image_to_tensor('right/exp_3.jpg',1).reshape(1,224,224,3)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "siamese_model.save_weights('siamese_final.h5')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Siam_Exp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
